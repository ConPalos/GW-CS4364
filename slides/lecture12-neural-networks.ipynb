{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/gw_monogram_2c.png\"></left>\n",
    "\n",
    "# Lecture 12: Neural Networks\n",
    "\n",
    "### Applied Machine Learning\n",
    "\n",
    "__Armin Mehrabina__<br>\n",
    "__Sardar Hamidian__<br>The George Washington Universiry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: An Artifical Neuron\n",
    "\n",
    "In this lecture, we will learn about a new class of machine learning algorithms inspired by the brain.\n",
    "\n",
    "We will start by defining a few building blocks for these algorithms, and draw connections to neuroscience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Review: Components of A Supervised Machine Learning Problem\n",
    "\n",
    "At a high level, a supervised machine learning problem has the following structure:\n",
    "\n",
    "$$ \\underbrace{\\text{Training Dataset}}_\\text{Attributes + Features} + \\underbrace{\\text{Learning Algorithm}}_\\text{Model Class + Objective + Optimizer } \\to \\text{Predictive Model} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review: Binary Classification\n",
    "\n",
    "In supervised learning, we fit a model of the form\n",
    "$$ f : \\mathcal{X} \\to \\mathcal{Y} $$\n",
    "that maps inputs $x \\in \\mathcal{X}$ to targets $y \\in \\mathcal{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In classification, the space of targets $\\mathcal{Y}$ is *discrete*. Classification is binary if $\\mathcal{Y} = \\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Biological Neuron\n",
    "\n",
    "In order to define an artifical neuron, let's look first at a biological one.\n",
    "\n",
    "<center>\n",
    "<img width=60% src=\"img/nns/bio-neuron.png\">\n",
    "</center>\n",
    "\n",
    "* Each neuron receives input signals from its dendrites\n",
    "* If input signals are strong enough, neuron fires output along its axon, which connects to the dendrites of other neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Artificial Neuron: Example\n",
    "\n",
    "We can imitate this machinery using an idealized artifical neuron.\n",
    "* Dendrite $j$ gets signal $x_j$; modulates multiplicatively to $w_j \\cdot x_j$.\n",
    "* The body of the neuron sums the modulated inputs: $\\sum_{j=1}^d w_j \\cdot x_j$.\n",
    "* These go into the activation function that produces an ouput.\n",
    "<center>\n",
    "<img width=\"55%\" src=\"img/nns/aneuron.jpeg\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Artificial Neuron: Notation\n",
    "\n",
    "More formally, we say that a neuron is a model $f : \\mathbb{R}^d \\to [0,1]$, with the following components:\n",
    "* Inputs $x_1,x_2,...,x_d$, denoted by a vector $x$.\n",
    "* Weight vector $w \\in \\mathbb{R}^d$ that modulates input $x$ as $w^\\top x$.\n",
    "* An activation function $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ that computes the output $\\sigma(w^\\top x)$ of the neuron based on the sum of modulated features $w^\\top x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "If we use a step function as the activation function, we obtain the classic Perceptron model:\n",
    "\n",
    "$$ f(x) = \n",
    "\\begin{cases}\n",
    "  1 & \\text{if $\\theta^\\top x>0$}, \\\\\n",
    "  0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This models a neuron that fires if the inputs are sufficiently large, and doesn't otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can visualize the activation function of the Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m step_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m z \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(z, [step_fn(zi) \u001b[38;5;28;01mfor\u001b[39;00m zi \u001b[38;5;129;01min\u001b[39;00m z])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "step_fn = lambda z: 1 if z > 0 else 0\n",
    "plt.plot(z, [step_fn(zi) for zi in z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression as an Artifical Neuron\n",
    "\n",
    "Logistic regression is a model of the form\n",
    "$$ f(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}, $$\n",
    "that can be interpreted as a neuron that uses the *sigmoid* as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The sigmoid activation function encodes the idea of a neuron firing if the inputs exceed a threshold, makes make the activation function \"smooth\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5)\n",
    "sigma = 1/(1+np.exp(-z))\n",
    "\n",
    "plt.plot(z, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activation Functions\n",
    "\n",
    "There are many other activation functions that can be used. In practice, these two work better than the sigmoid:\n",
    "* Hyperbolic tangent (`tanh`): $\\sigma(z) = \\tanh(z)$\n",
    "* Rectified linear unit (`ReLU`): $\\sigma(z) = \\max(0, z)$\n",
    "<!-- * Leaky `ReLU`: $$\\sigma(z) = \\max(\\alpha z, z),$$ where $\\alpha$ is a small constant such as 0.1. -->\n",
    "\n",
    "We can easily visualize these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.tanh(z))\n",
    "plt.subplot(122)\n",
    "plt.plot(z, np.maximum(z, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification Dataset: Iris Flowers\n",
    "\n",
    "To demonstrate classification algorithms, we are going to use the Iris flower dataset. \n",
    "\n",
    "We are going to define an artificial neuron for the binary classification problem (class-0 vs the rest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "iris_X, iris_y = iris.data, iris.target\n",
    "\n",
    "# rename class two to class one\n",
    "iris_y2 = iris_y.copy()\n",
    "iris_y2[iris_y2==2] = 1\n",
    "\n",
    "X = iris_X.to_numpy()[:,:2]\n",
    "Y = iris_y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is a visualization of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot also the training points\n",
    "p1 = plt.scatter(X[:,0], X[:,1], c=iris_y2, edgecolor='k', s=60, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Non-Setosa'], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below, we define neuron with a sigmoid activation function (and its gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def neuron(X, theta):\n",
    "    activation_fn = lambda z: 1/(1+np.exp(-z))\n",
    "    return activation_fn(X.dot(theta))\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    return np.mean((y - neuron(X, theta)) * X.T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can optimize is using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 5e-5\n",
    "step_size = 1e-1\n",
    "\n",
    "iter, theta, theta_prev = np.zeros((3,)), np.ones((3,)), 0\n",
    "iris_X['one'] = 1 # add a vector of ones for the bias\n",
    "X_train = iris_X.iloc[:,[0,1,-1]].to_numpy()\n",
    "y_train = iris_y2.to_numpy()\n",
    "\n",
    "while np.linalg.norm(theta - theta_prev) > threshold:\n",
    "    if iter % 50000 == 0:\n",
    "        print('Iteration %d.' % iter)\n",
    "    theta_prev = theta\n",
    "    grad = gradient(theta, X_train, y_train)\n",
    "    theta = theta_prev + step_size * grad\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This neuron learns a linear decision boundary that separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions over a grid:\n",
    "xx, yy = np.meshgrid(np.arange(3.3, 8.9, 0.02), np.arange(1.0, 5.4, 0.02))\n",
    "Z = neuron(np.c_[xx.ravel(), yy.ravel(), np.ones(xx.ravel().shape)], theta)\n",
    "Z[Z<0.5] = 0\n",
    "Z[Z>=0.5] = 1\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: Artificial Neuron\n",
    "\n",
    "* __Type__: Supervised learning (regression and classification).\n",
    "* __Model family__: Linear model followed by non-linear activation.\n",
    "* __Objective function__: Any differentiable objective.\n",
    "* __Optimizer__: Gradient descent.\n",
    "* __Special Cases__: Logistic regression, Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/gw_monogram_2c.png\"></left>\n",
    "# Part 2: Artificial Neural Networks\n",
    "\n",
    "Let's now see how we can connect neurons into networks that form complex models that further mimic the brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Review: Artificial Neuron\n",
    "\n",
    "We say that a neuron is a model $f : \\mathbb{R}^d \\to [0,1]$, with the following components:\n",
    "* Inputs $x_1,x_2,...,x_d$, denoted by a vector $x$.\n",
    "* Weight vector $w \\in \\mathbb{R}^d$ that modulates input $x$ as $w^\\top x$.\n",
    "* An activation function $\\sigma: \\mathbb{R} \\to \\mathbb{R}$ that computes the output $\\sigma(w^\\top x)$ of the neuron based on the sum of modulated features $w^\\top x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Review: Logistic Regression as Neuron\n",
    "\n",
    "Logistic regression is a model of the form\n",
    "$$ f(x) = \\sigma(\\theta^\\top x) = \\frac{1}{1 + \\exp(-\\theta^\\top x)}, $$\n",
    "that can be interpreted as a neuron that uses the *sigmoid* as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: Intuition\n",
    "\n",
    "A neural network is a directed graph in which a node is a neuron that takes as input the outputs of the neurons that are connected to it.\n",
    "<center>\n",
    "<img src=\"img/nns/net-intro.png\">\n",
    "</center>\n",
    "Networks are typically organized in layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: Layers\n",
    "\n",
    "A neural network layer is a model $f : \\mathbb{R}^d \\to \\mathbb{R}^p$ that applies $p$ neurons in parallel to an input $x$.\n",
    "$$ f(x) = \\begin{bmatrix}\n",
    "\\sigma(w_1^\\top x) \\\\\n",
    "\\sigma(w_2^\\top x) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(w_p^\\top x)\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "where each $w_k$ is the vector of weights for the $k$-th neuron. We refer to $p$ as the *size* of the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first output of the layer is a neuron with weights $w_1$:\n",
    "<center>\n",
    "<img src=\"img/nns/layers-1.png\" width=60%>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second neuron has weights $w_2$:\n",
    "<center>\n",
    "<img src=\"img/nns/layers-2.png\" width=60%>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The third neuron has weights $w_3$:\n",
    "<center>\n",
    "<img src=\"img/nns/layers-3.png\" width=60%>\n",
    "</center>    \n",
    "The parameters of the layer are $w_1, w_2, w_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By combining the $w_k$ into one matrix $W$, we can write in a more succinct vectorized form:\n",
    "$$f(x) = \\sigma(W\\cdot x) = \\begin{bmatrix}\n",
    "\\sigma(w_1^\\top x) \\\\\n",
    "\\sigma(w_2^\\top x) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(w_p^\\top x)\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where $\\sigma(W\\cdot x)_k = \\sigma(w_k^\\top x)$ and $W_{kj} = (w_k)_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Visually, we can represent this as follows:\n",
    "<center>\n",
    "<img src=\"img/nns/layers-4.png\" width=60%>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: Notation\n",
    "\n",
    "A neural network is a model $f : \\mathbb{R}^d \\to \\mathbb{R}$ that consists of a composition of $L$ neural network layers:\n",
    "$$ f(x) = f_L \\circ f_{L-1} \\circ \\ldots f_l \\circ \\ldots f_1 (x). $$\n",
    "The final layer $f_L$ has size one (assuming the neural net has one ouput); intermediary layers $f_l$ can have any number of neurons.\n",
    "\n",
    "The notation $f \\circ g(x)$ denotes the composition $f(g(x))$ of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can visualize this graphically as follows.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/nns/layers-L.png\" width=100%>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example of a Neural Network\n",
    "\n",
    "Let's implement a small two layer neural net with 3 hidden units.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/nns/nn-example.png\" width=60%>\n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This implementation looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# a two layer network with logistic function as activation\n",
    "class Net():\n",
    "    def __init__(self, x_dim, W_dim):\n",
    "        # weight matrix for layer 1\n",
    "        self.W = np.random.normal(size=(x_dim, W_dim))\n",
    "        # weight matrix for layer 2, also the output layer\n",
    "        self.V = np.random.normal(size=(W_dim, 1))\n",
    "        # activation function\n",
    "        self.afunc = lambda x: 1/(1+np.exp(-x))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # get output of the first layer\n",
    "        l1 = self.afunc(np.matmul(x, self.W))\n",
    "        # get output of the second layer, also the output layer\n",
    "        out = self.afunc(np.matmul(l1, self.V))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Later in this lecture, we will see how to train this model using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of Neural Network Layers\n",
    "\n",
    "There are many types of neural network layers that can exist. Here are a few:\n",
    "* Ouput layer: normally has one neuron and special activation function that depends on the problem\n",
    "* Input layer: normally, this is just the input vector $x$.\n",
    "* Hidden layer: Any layer between input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dense layer: A layer in which every input is connected to every neuron.\n",
    "* Convolutional layer: A layer in which the operation $w^\\top x$ implements a mathematical [convolution](https://en.wikipedia.org/wiki/Convolution).\n",
    "* Recurrent Layer: A layer in which a neuron's output is connected back to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm: (Fully-Connected) Neural Network\n",
    "\n",
    "* __Type__: Supervised learning (regression and classification).\n",
    "* __Model family__: Compositions of layers of artificial neurons.\n",
    "* __Objective function__: Any differentiable objective.\n",
    "* __Optimizer__: Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pros and Cons of Neural Nets\n",
    "\n",
    "Neural networks are very powerful models.\n",
    "* They are flexible, and can approximate any function.\n",
    "* They work well over unstructured inputs like audio or images.\n",
    "* They can achieve state-of-the-art perfomrance.\n",
    "\n",
    "They also have important drawbacks.\n",
    "* They can also be slow and hard to train.\n",
    "* Large neworks require a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<left><img width=25% src=\"img/gw_monogram_2c.png\"></left>\n",
    "# Part 3: Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm for efficiently computing the gradient of multi-layer neural network in order to train the network using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivating Example: A Toy Network\n",
    "\n",
    "Consider the following operations that output $e$ from inputs $a, b$:\n",
    "\\begin{align*}\n",
    "c = a + b && d = b + 1 && e = c \\cdot d\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can represent this as a computational graph (figures by [Chris Olah](http://colah.github.io/posts/2015-08-Backprop)):\n",
    "\n",
    "<center>\n",
    "<img src=\"img/olah_graph1.png\" width=50%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's say we want to compute $e$ from $a=2, b=1$. We can start from the leaves and work our way up as follows:\n",
    "\n",
    "<center>\n",
    "<img src=\"img/olah_graph2.png\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Chain Rule of Calculus\n",
    "\n",
    "Suppose that we now want to compute derivatives within this computational graph. We can leverage the chain rule of calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we have two differentiable functions $f(x)$ and $g(x)$, and $$F(x) = f \\circ g (x)$$ then the derivative of $F(x)$ is:\n",
    "$$ F^\\prime (x) = f^\\prime (g(x)) \\cdot g^\\prime (x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $y=f(u)$ and $u=g(x)$, we also have:\n",
    "$$ \\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derivatives in Our Toy Example\n",
    "\n",
    "Consider our toy example. We can add the derivative of the output of each node with respect to its input along each edge of the graph.\n",
    "<center>\n",
    "<img src=\"img/olah_graph3.png\" width=70%>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"img/olah_graph3.png\" width=70%>\n",
    "</center>\n",
    "If we want to compute the derivative of the output with respect to the input, we can multiply the partial derivatives along its path: $$\\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c} \\frac{\\partial c}{\\partial a} = 2 \\cdot 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"img/olah_graph3.png\" width=70%>\n",
    "</center>\n",
    "\n",
    "If we have multiple paths to the root, we sum them all (a total derivative!):\n",
    "$$\\frac{\\partial e}{\\partial b} = \\frac{\\partial e}{\\partial c} \\frac{\\partial c}{\\partial b} + \\frac{\\partial e}{\\partial d} \\frac{\\partial d}{\\partial b} = 2 \\cdot 1 + 3 \\cdot 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key ideas here are that:\n",
    "* After performing one feed-forward pass to compute node values and one backward pass to compute edge derivatives, we can compute arbitrary derivatives.\n",
    "* The two passes precompute information that lets us calculate derivatives very efficiently (in linear time). This is an example of *dynamic programming*.\n",
    "\n",
    "The resulting algorithm is called *backpropagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Derivatives in Neural Networks\n",
    "\n",
    "Recall that a neural network is a model $f : \\mathbb{R} \\to \\mathbb{R}$ that consists of a composition of $L$ neural network layers:\n",
    "$$ f(x) = f_L \\circ f_{L-1} \\circ \\ldots f_1 (x). $$\n",
    "The final layer $f_L$ has size one (assuming the neural net has one ouput); intermediary layers $f_l$ can have any number of neurons.\n",
    "\n",
    "The notation $f \\circ g(x)$ denotes the composition $f(g(x))$ of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A neural network layer is a model $f_l : \\mathbb{R}^d \\to \\mathbb{R}^p$ that applies $p$ neurons in parallel to an input $x$.\n",
    "$$f_l(x) = \\sigma(W_l\\cdot x) = \\begin{bmatrix}\n",
    "\\sigma(w_{l1}^\\top x) \\\\\n",
    "\\sigma(w_{l2}^\\top x) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(w_{lp}^\\top x)\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "where each $w_{lk}$ is the vector of weights for the $k$-th neuron. We want to compute the derivatives of $J$ with respect to all the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let $J$ be the objective function of the neural network. We are going to be computing the following derivatives:\n",
    "* $\\frac{\\partial J}{\\partial w_{lkj}}$, the derivative with respect to each weight $w_{lkj}$.\n",
    "* $\\frac{\\partial J}{\\partial y_{lj}}$, the derivative with respect to the output of the $j$-th neuron in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Review: The Gradient\n",
    "\n",
    "The gradient $\\nabla_\\theta f$ further extends the derivative to multivariate functions $f : \\mathbb{R}^d \\to \\mathbb{R}$, and is defined at a point $\\theta$ as\n",
    "$$ \\nabla_\\theta f (\\theta) = \\begin{bmatrix}\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f(\\theta)}{\\partial \\theta_d}\n",
    "\\end{bmatrix}.$$\n",
    "In other words, the $j$-th entry of the vector $\\nabla_\\theta f (\\theta)$ is the partial derivative $\\frac{\\partial f(\\theta)}{\\partial \\theta_j}$ of $f$ with respect to the $j$-th component of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Review: Gradient Descent\n",
    "\n",
    "If we want to optimize an objective $J(\\theta)$, we start with an initial guess $\\theta_0$ for the parameters and repeat the following update until the function is no longer decreasing:\n",
    "$$ \\theta_i := \\theta_{i-1} - \\alpha \\cdot \\nabla_\\theta J(\\theta_{i-1}). $$\n",
    "\n",
    "As code, this method may look as follows:\n",
    "```python\n",
    "theta, theta_prev = random_initialization()\n",
    "while norm(theta - theta_prev) > convergence_threshold:\n",
    "    theta_prev = theta\n",
    "    theta = theta_prev - step_size * gradient(theta_prev)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm for computing all $\\partial J / \\partial{w_{lkj}}$ in two steps:\n",
    "1. In the forward pass, we start from the input $x$ and compute the output $y_l$ of each layer $f_l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. In the backward pass, we start from the top and recursively compute the following partial derivatives:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial y_{lk}} && \\frac{\\partial J}{\\partial w_{lkj}},\n",
    "\\end{align*}\n",
    "where $y_{lk}$ is the output of the $k$-th neuron in layer $l$, and  $\\partial w_{lkj}$ is the $kj$-th weight in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can visualize this graphically as follows. This is a full neural network:\n",
    "\n",
    "<img src=\"img/nns/layers-L.png\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is a zoom-in on two consecutive layers:\n",
    "<center>\n",
    "<img src=\"img/nns/layers-3b.png\" width=70%>\n",
    "</center>\n",
    "Note that the input to layer $f_{l+1}$ is $y_l$, the output of layer $f_l$. We use respectively $k$ and $j$ to index the neurons in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We compute the two sets of partial derivatives using the following recursive formulas for $l = L-1,L-2,...,1$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial y_{lj}} & = \\sum_{k=1}^p \\frac{\\partial J}{\\partial y_{l+1, k}} \\cdot \\frac{\\partial y_{l+1, k}}{\\partial y_{lj}}  && \\text{(total derivative})\\\\\n",
    "\\frac{\\partial J}{\\partial w_{lkj}} & = \\frac{\\partial J}{\\partial y_{lk}} \\cdot \\frac{\\partial y_{lk}}{\\partial w_{lkj}} && \\text{(chain rule})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that $\\frac{\\partial y_{lk}}{\\partial w_{lkj}}$ and $\\frac{\\partial y_{l+1, k}}{\\partial y_{lj}}$ are local derivatives of a neuron (can be easily computed) and the $\\frac{\\partial J}{\\partial y_{l+1, k}}$ are available from the previous step. When $l=L$, these  are easy-to-compute derivatives the output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We perform this procedure recursively from the top of the network to the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backprogragation by Hand\n",
    "\n",
    "Let's work out by hand what backpropagation would do on our two layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img width=\"450\" src=\"img/nns/backprop/backprop-twolayers-figure0.png\">\n",
    "</center>    \n",
    "\n",
    "For our two layer fully connected network with sigmoid activation, the network is composed of following functions:\n",
    "\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{W}^T \\mathbf{x})$$\n",
    "$$y = \\sigma(\\mathbf{V}^T \\mathbf{h}),$$\n",
    "\n",
    "where $\\mathbf{x} = [x_1,x_2]^T, \\mathbf{h} = [h_1,h_2,h_3]^T, \\mathbf{W} \\in \\mathbb{R}^{2\\times3}, \\mathbf{V} \\in \\mathbb{R}^{3\\times1}$, and $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In our example, we have the following values:\n",
    "\n",
    "$\\mathbf{x} = [5.0,3.0]^T,~~~~\\hat{y} = 1$ means it is positive class.\n",
    "\n",
    "$\\mathbf{W} = \\begin{bmatrix}\n",
    "1.0 & -1.0 & 3.0\\\\ \n",
    "2.0 & 2.0 & -1.0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf{V} = [0.1,0.5,-0.1]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure1.png\">\n",
    "We can compute the output of the hidden layer, $\\mathbf{h}$:\n",
    "\n",
    "$h_1 = \\sigma (W_{11} \\cdot x_1 + W_{21} \\cdot x_2) = \\sigma (1.0\\times5.0 + 2.0\\times3.0) = 0.9999$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure2.png\">\n",
    "We can compute the output of the hidden layer, $\\mathbf{h}$:\n",
    "\n",
    "\\begin{align*}\n",
    "h_2 &= \\sigma (W_{12} \\cdot x_1 + W_{22} \\cdot x_2) = \\sigma (-1.0\\times5.0 + 2.0\\times3.0) = 0.7310\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure3.png\">\n",
    "We can compute the output of the hidden layer, $\\mathbf{h}$:\n",
    "\n",
    "\\begin{align*}\n",
    "h_3 &= \\sigma (W_{13} \\cdot x_1 + W_{23} \\cdot x_2) = \\sigma (3.0\\times5.0 + -1.0\\times3.0) = 0.9999\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure4.png\">\n",
    "Similarly we can get the output of $y$:\n",
    "\n",
    "$$y = \\sigma (V_1 \\cdot h_1 + V_2 \\cdot h_2 + V_3 \\cdot h_3) = 0.590378$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, we want to compute the gradient with respect to each of the parameters of this network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider first computing the gradients of the weights in the output layer:\n",
    "$$\\frac{\\mathrm{d}J}{\\mathrm{d}{V}} = \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\mathrm{d}y}{\\mathrm{d}{V}}$$\n",
    "In order to compute $\\frac{\\mathrm{d}J}{\\mathrm{d}{V}}$ we can separately compute $\\frac{\\mathrm{d}J}{\\mathrm{d}{y}}$ and $\\frac{\\mathrm{d}y}{\\mathrm{d}{V}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start with $\\frac{\\mathrm{d}J}{\\mathrm{d}{y}}$. Recall that the binary cross-entropy loss is:\n",
    "\\begin{align*}\n",
    "J(y, \\hat{y}) = - \\hat{y} \\cdot \\log (y) - (1-\\hat{y})\\cdot (1-\\log (y))\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The derivative with respect to $y$ when $\\hat y = 1$ and $y=0.59$ is:\n",
    "\\begin{align*}\n",
    "J(y, \\hat{y}) = - \\log (y) = 0.5269\n",
    "&&\n",
    "\\frac{\\mathrm{d}J}{\\mathrm{d}{y}} = - 1/y = -1.6938\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will start denoting in red the derivatives $\\partial J / \\partial v$ of the objective $J$ with respect to the each variable $v$.\n",
    "\n",
    "<img width=\"1000\" src=\"img/nns/backprop/backprop-twolayers-figure5.png\">\n",
    "\n",
    "Above, we have added in red $\\partial J / \\partial y$ at node $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we want to compute $\\frac{\\mathrm{d}y}{\\mathrm{d}{V}}$.\n",
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure5.png\">\n",
    "Recall: $y = \\sigma(\\mathbf{V}^T \\mathbf{h}) = \\sigma (V_1 h_1 + V_2 h_2 + V_3 h_3)$ and $\\sigma' = \\sigma(1-\\sigma)$:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial V_1} = y (1-y) h_1,~~\\frac{\\partial y}{\\partial V_2} = y (1-y) h_2,~~\\frac{\\partial y}{\\partial V_3} = y (1-y) h_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- <center>\n",
    "<img width=\"700\" src=\"img/nns/backprop/backprop-twolayers-figure6.png\">\n",
    "</center> -->\n",
    "Applying these formulas, we obtain the gradients of $\\mathbf{V}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial V_1} &= \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\partial y}{\\partial V_1} = -1.69 \\times 0.59 \\times (1-0.59) \\times 0.99998 = -0.41\\\\\n",
    "\\frac{\\partial J}{\\partial V_2} &= \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\partial y}{\\partial V_2} = -1.69 \\times 0.59 \\times (1-\n",
    "0.59) \\times 0.7311 = -0.30\\\\\n",
    "\\frac{\\partial J}{\\partial V_3} &= \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\partial y}{\\partial V_3} = -1.69 \\times 0.59 \\times (1-0.59) \\times 0.99999 = -0.41\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure6.png\">\n",
    "We denote these in orange on the edges of the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, let's compute gradients at the hidden layer:\n",
    "$$\\frac{\\mathrm{d}J}{\\mathrm{d}{h}} = \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\mathrm{d}y}{\\mathrm{d}{h}}$$\n",
    "\n",
    "Similarly to the previous slide:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial h_1} &= \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\partial y}{\\partial h_1} = -1.69 \\times 0.59 \\times (1-0.59) \\times 0.1 = -0.04096\\\\\n",
    "\\frac{\\partial J}{\\partial h_2} &= \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\partial y}{\\partial h_2} = -1.69 \\times 0.59 \\times (1-\n",
    "0.59) \\times 0.5 = -0.2048\\\\\n",
    "\\frac{\\partial J}{\\partial h_3} &= \\frac{\\mathrm{d}J}{\\mathrm{d}{y}} \\frac{\\partial y}{\\partial h_3} = -1.69 \\times 0.59 \\times (1-0.59) \\times -0.1 = 0.04096\n",
    "\\end{align*}\n",
    "<!-- From now on we can ignore the output layer, as we have everything we need to compute gradients in layers ahead. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure7.png\">\n",
    "We add these in red on the graph. Crucially, all the downstream derivatives can be computed from these derivatives without using any upstream nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we have another linear layer with sigmoid activation, the way we compute gradients will be the same as before:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial h_1} \\frac{\\partial h_1}{\\partial W_{11}} &= -0.041 \\times 0.99998 \\times (1-0.99998) \\times 5 = -3\\times10^{-6} \\\\\n",
    "\\frac{\\mathrm{d}J}{\\mathrm{d}{h_1}} \\frac{\\partial h_1}{\\partial W_{12}} &= -0.041 \\times 0.99998 \\times (1-\n",
    "0.99998) \\times 3 = -2\\times10^{-6}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure8.png\">\n",
    "We denote these in purple on the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img width=\"900\" src=\"img/nns/backprop/backprop-twolayers-figure9.png\">\n",
    "We can compute all the other gradients in the same way.\n",
    "\n",
    "<!-- Note the gradients to the weights connecting to $h_2$ are larger in magnitude than others.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<img width=\"700\" src=\"img/nns/backprop/backprop-twolayers-figure10.png\">\n",
    "</center>\n",
    "\n",
    "And now we have the gradients to all the learnable weights in this two layer network and we can tune the weights by gradient descent.\n",
    "\n",
    "The gradients tell us how much to change for each weight so that the loss will become smaller. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation in Numpy\n",
    "\n",
    "Now let's implement backprop with the simple neural network model we defined earlier.\n",
    "\n",
    "We start by implementing the building block of our network: a linear layer with sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# a single linear layer with sigmoid activation\n",
    "class LinearSigmoidLayer():\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.W = np.random.normal(size=(in_dim,out_dim))\n",
    "        self.W_grad = np.zeros_like(self.W)\n",
    "        \n",
    "        self.afunc = lambda x: 1. / (1. + np.exp(-x))\n",
    "    \n",
    "    # forward function to get output\n",
    "    def forward(self, x):\n",
    "        Wx = np.matmul(x, self.W)\n",
    "        self.y = self.afunc(Wx)\n",
    "        self.x = x\n",
    "        return self.y        \n",
    "        \n",
    "    # backward function to compute gradients\n",
    "    # grad_out is dJ/dy, where y is the layer's output\n",
    "    # grad_in is dJ/dx, where x is the layer's output\n",
    "    # the gradient dJ/dW is saved to self.W_grad\n",
    "    def backward(self, grad_out):  \n",
    "        self.W_grad = np.matmul(\n",
    "            self.x.transpose(), \n",
    "            self.y * (1-self.y) * grad_out,\n",
    "            )\n",
    "        grad_in = np.matmul(\n",
    "            self.y * (1-self.y) * grad_out,\n",
    "            self.W.transpose()\n",
    "            )\n",
    "        \n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we can stack the single layers to construct a two layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# a two layer network with logistic function as activation\n",
    "class Net():\n",
    "    def __init__(self, x_dim, W_dim):\n",
    "        self.l1 = LinearSigmoidLayer(x_dim, W_dim)\n",
    "        self.l2 = LinearSigmoidLayer(W_dim, 1)\n",
    "    \n",
    "    # get output\n",
    "    def predict(self, x):\n",
    "        h = self.l1.forward(x)\n",
    "        self.y = self.l2.forward(h)\n",
    "        return self.y\n",
    "    \n",
    "    # backprop\n",
    "    def backward(self, label):\n",
    "        # binary cross entropy loss, and gradients\n",
    "        if label == 1:\n",
    "            J = -1*np.log(self.y)\n",
    "            dJ = -1/self.y\n",
    "        else:\n",
    "            J = -1*np.log(1-self.y)\n",
    "            dJ = 1/(1-self.y)\n",
    "            \n",
    "        # back propagation\n",
    "        dJdh = self.l2.backward(dJ) # output --> hidden\n",
    "        dJdx = self.l1.backward(dJdh) # hidden --> input\n",
    "        return J\n",
    "    \n",
    "    # update weights according to gradients\n",
    "    def grad_step(self, lr=1e-4):\n",
    "        self.l1.W -= lr*self.l1.W_grad\n",
    "        self.l2.W -= lr*self.l2.W_grad     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can run with our previous example to check if the results are consistent with our manual computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Net(2, 3)\n",
    "model.l1.W = np.array([[1.0,-1.0,3.0],[2.0,2.0,-1.0]])\n",
    "model.l2.W = np.array([[0.1],[0.5],[-0.1]])\n",
    "\n",
    "x = np.array([5.0, 3.0])[np.newaxis,...]\n",
    "x_label = 1\n",
    "\n",
    "# forward\n",
    "out = model.predict(x)\n",
    "\n",
    "# backward\n",
    "loss = model.backward(label=x_label)\n",
    "\n",
    "print('loss: {}'.format(loss))\n",
    "print('W grad: {}'.format(model.l1.W_grad))\n",
    "print('V grad: {}'.format(model.l2.W_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another sanity check is to perform gradient descent on the single sample input and see if we can achieve close to zero loss.\n",
    "\n",
    "You can try to change the target label below to see the network is able to adapt in either case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## gradient descent\n",
    "loss = []\n",
    "score = []\n",
    "for i in range(100):\n",
    "    out = model.predict(x)\n",
    "    loss.append(model.backward(label=1)) # 1 for positive, 0 for negative\n",
    "    model.grad_step(lr=1e-1)\n",
    "    score.append(out)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(loss).squeeze(),'-')\n",
    "plt.plot(np.array(score).squeeze(),'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Neural networks are powerful models that can approximate any function.\n",
    "* They are trained using gradient descent.\n",
    "* In order to compute gradients, we use an efficient algorithm called backpropagation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "neural-ode.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "rise": {
   "controlsTutorial": false,
   "height": 900,
   "help": false,
   "margin": 0,
   "maxScale": 2,
   "minScale": 0.2,
   "progress": true,
   "scroll": true,
   "theme": "simple",
   "width": 1200
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
